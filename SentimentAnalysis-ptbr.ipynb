{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b4ef4c-21cd-428c-9464-8db30b867d40",
   "metadata": {},
   "source": [
    "%%capture \n",
    "!pip install import-ipynb \n",
    "!pip install textblob \n",
    "!pip install spacy \n",
    "!pip install nltk \n",
    "!pip install -U sklearn \n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ancient-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import glob\n",
    "\n",
    "from leia import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from googletrans import Translator\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526d2ef1-803c-4fcd-a7da-e6f1e82807fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.2.0/pt_core_news_sm-3.2.0-py3-none-any.whl (22.2 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rodox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rodox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Execute de line below to download de 'pt_core_news_sm'\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "nltk.download('stopwords')\n",
    "sw = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c89906-5207-4624-ae5a-16be6ff5c691",
   "metadata": {},
   "source": [
    "## Prepeocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e63566-6b63-4776-920a-ab64869de470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tt_username(text):\n",
    "    text = str(text)\n",
    "    no_tt_username = re.sub(r'\\@\\S+', '', text)\n",
    "    return no_tt_username\n",
    "\n",
    "def identify_emoticons(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\:\\-?\\)+', ' cara feliz ', text)\n",
    "    text = re.sub(r'\\:\\-?[dDpP]+', ' cara feliz ', text)\n",
    "    text = re.sub(r'\\:\\-?\\'?\\(+', ' cara triste ', text)\n",
    "    text = re.sub(r'\\>\\:\\-?\\(+', ' cara brava ', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = str(text)\n",
    "    no_hashtags = re.sub(r'\\#\\S+', '', text)\n",
    "    return no_hashtags\n",
    "\n",
    "def remove_phone(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'(\\(?(\\d{2,3})\\)?)?\\ ?\\d{4,5}\\-?\\ ?\\d{4}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_url(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?\\:\\/\\/\\S+', ' ', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[a-zA-Z|.]+\\.com(\\.br)?', ' link ', text)\n",
    "    return text\n",
    "\n",
    "def remove_date(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'((\\d{1,2}\\/)(\\d{1,2}\\/?)(\\d{2,4})?)', ' ', text)\n",
    "    text = re.sub(r'((\\d{1,2}\\-)(\\d{1,2}\\-?)(\\d{2,4})?)', ' ', text)\n",
    "    text = re.sub(r'((\\d+(\\s+[deDE]+\\s+)[aA-zZ|ç|Ç]+((\\s+[deDE]+\\s+)\\d+)?))', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_hour(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'(\\d+)\\:(\\d+)[hH]?(\\:\\d+)?[hH]?[rsRS]\\w?', ' ', text)\n",
    "    text = re.sub(r'(\\d+)[hH](\\d+)', ' < hora > ', text)\n",
    "    return text\n",
    "\n",
    "def remove_number(text): \n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_oneword(text):\n",
    "    text = str(text)\n",
    "    if len(text.split()) > 1:        \n",
    "        return text\n",
    "    return\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text = str(text) \n",
    "    text = [word for word in text.split() if word not in sw]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = str(text) \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode(\"utf-8\")\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    text = str(text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U000E007F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\" \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def remove_punction(text): \n",
    "    text = str(text) \n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-.º<>/:;=?@[/\\/\\]^_`{|}~]', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = pd.Series(data)\n",
    "    data = data.apply(remove_tt_username)\n",
    "    data = data.apply(remove_hashtags)\n",
    "    data = data.apply(identify_emoticons)\n",
    "    data = data.apply(remove_url)\n",
    "    data = data.apply(remove_phone)\n",
    "    data = data.apply(remove_hour)\n",
    "    data = data.apply(remove_date)\n",
    "    data = data.apply(remove_number)\n",
    "    data = data.apply(remove_emoji)\n",
    "    data = data.apply(lowercase)\n",
    "    data = data.apply(remove_stopword)\n",
    "    data = data.apply(remove_accent)\n",
    "    data = data.apply(remove_punction)\n",
    "    data = data.apply(remove_oneword)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939468c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c723cf",
   "metadata": {},
   "source": [
    "### LeIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ed0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def leia(text):\n",
    "    text = str(text)\n",
    "    result = analyzer.polarity_scores(text)\n",
    "    \n",
    "    #analisa a frase utilizando o compound\n",
    "    if result['compound'] >= 0.05:\n",
    "        return 'positivo'\n",
    "    elif result['compound'] <= -0.05:\n",
    "        return 'negativo'\n",
    "    else:\n",
    "        return 'neutro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd81173",
   "metadata": {},
   "source": [
    "### TextBlob + ReLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d2e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob(sentence):\n",
    "    sentence = str(sentence)\n",
    "    blob = TextBlob(sentence)\n",
    "    result = 0\n",
    "    \n",
    "    #translate the text to english\n",
    "    translation = translator.translate(sentence, src='pt', dest='en')\n",
    "    translation = translation.text\n",
    "    translation = TextBlob(translation)\n",
    "    result = translation.sentiment.polarity\n",
    "        \n",
    "    if result > 0:\n",
    "        return 'positivo'\n",
    "    elif result < 0:\n",
    "        return 'negativo'\n",
    "    else:\n",
    "        return 'neutro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47993bef",
   "metadata": {},
   "source": [
    "### OpLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558294b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lexico_v3.0.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [str(x.strip()) for x in lines]\n",
    "pol_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    word, _, pol, _ = line.split(',')\n",
    "    \n",
    "    if word not in pol_dict.keys():\n",
    "        pol_dict[word] = pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3a6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oplexion(text):\n",
    "    text = str(text)\n",
    "    doc = nlp(text)\n",
    "    pol = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in pol_dict.keys():\n",
    "            if token.pos_ == 'VERB':\n",
    "                if token.lemma_ in pol_dict.keys():\n",
    "                    pol += int(pol_dict[str(token.lemma_)])\n",
    "                else:\n",
    "                    pol += int(pol_dict[str(token.text)])\n",
    "            else:\n",
    "                pol += int(pol_dict[str(token.text)])\n",
    "        else:\n",
    "            pol += 0\n",
    "        \n",
    "    if pol > 0:\n",
    "        return 'positivo'\n",
    "    elif pol < 0:\n",
    "        return 'negativo'\n",
    "    else:\n",
    "        return 'neutro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33f64b",
   "metadata": {},
   "source": [
    "### SentiLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1124a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SentiLex-lem-PT01.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [str(x.strip()) for x in lines]\n",
    "pol_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    word, infos = line.split('.')\n",
    "    pol = infos.split(';')\n",
    "    pol = pol[3]\n",
    "    pol = pol[4:]\n",
    "    \n",
    "    if word not in pol_dict.keys():\n",
    "        pol_dict[word] = pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77400c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentilex(text):\n",
    "    text = str(text)\n",
    "    doc = nlp(text)\n",
    "    pol = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        try:\n",
    "            if token.pos_ == 'VERB':\n",
    "                pol += int(pol_dict[str(token.lemma_)])\n",
    "            else:\n",
    "                pol += int(pol_dict[str(token.text)])\n",
    "        except KeyError:\n",
    "            pol += 0\n",
    "        \n",
    "    if pol > 0:\n",
    "        return 'positivo'\n",
    "    elif pol < 0:\n",
    "        return 'negativo'\n",
    "    else:\n",
    "        return 'neutro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "helpful-clerk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1031761728445530112</td>\n",
       "      <td>@Tixaa23 14 para eu ir :)</td>\n",
       "      <td>Tue Aug 21 04:35:39 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1031761040462278656</td>\n",
       "      <td>@drexalvarez O meu like eu já dei na época :)</td>\n",
       "      <td>Tue Aug 21 04:32:55 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1031760962372689920</td>\n",
       "      <td>Eu só queria conseguir comer alguma coisa pra ...</td>\n",
       "      <td>Tue Aug 21 04:32:37 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1031760948250456066</td>\n",
       "      <td>:D que lindo dia !</td>\n",
       "      <td>Tue Aug 21 04:32:33 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031760895985246208</td>\n",
       "      <td>@Primo_Resmungao Pq da pr jeito!!é uma \"oferta...</td>\n",
       "      <td>Tue Aug 21 04:32:21 +0000 2018</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         tweet_text  \\\n",
       "0  1031761728445530112                          @Tixaa23 14 para eu ir :)   \n",
       "1  1031761040462278656      @drexalvarez O meu like eu já dei na época :)   \n",
       "2  1031760962372689920  Eu só queria conseguir comer alguma coisa pra ...   \n",
       "3  1031760948250456066                                 :D que lindo dia !   \n",
       "4  1031760895985246208  @Primo_Resmungao Pq da pr jeito!!é uma \"oferta...   \n",
       "\n",
       "                       tweet_date sentiment query_used  \n",
       "0  Tue Aug 21 04:35:39 +0000 2018  Positivo         :)  \n",
       "1  Tue Aug 21 04:32:55 +0000 2018  Positivo         :)  \n",
       "2  Tue Aug 21 04:32:37 +0000 2018  Positivo         :)  \n",
       "3  Tue Aug 21 04:32:33 +0000 2018  Positivo         :)  \n",
       "4  Tue Aug 21 04:32:21 +0000 2018  Positivo         :)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.concat(map(pd.read_csv, glob.glob('raw_data\\\\*.csv')))[['Search', 'Tweet_Date', 'Original_Tweet']].reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "figured-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785814, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "norman-midnight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Tixaa23 14 para eu ir :)</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@drexalvarez O meu like eu já dei na época :)</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eu só queria conseguir comer alguma coisa pra ...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:D que lindo dia !</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Primo_Resmungao Pq da pr jeito!!é uma \"oferta...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    target\n",
       "0                          @Tixaa23 14 para eu ir :)  positivo\n",
       "1      @drexalvarez O meu like eu já dei na época :)  positivo\n",
       "2  Eu só queria conseguir comer alguma coisa pra ...  positivo\n",
       "3                                 :D que lindo dia !  positivo\n",
       "4  @Primo_Resmungao Pq da pr jeito!!é uma \"oferta...  positivo"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['id', 'tweet_date', 'query_used'])\n",
    "data = data.rename({'tweet_text': 'text', 'sentiment': 'target'}, axis=1)\n",
    "data['target'] = data['target'].map(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marine-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.7 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai quando a gente se conhecer eu vou ficar tão...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>ai gente conhecer vou ficar tao feliz aaaaaa v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@_iamSHUKRI Esp at 4:30 am :)))</td>\n",
       "      <td>positivo</td>\n",
       "      <td>esp at   am cara feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@eujulianaramosv Muito triste cara :(</td>\n",
       "      <td>negativo</td>\n",
       "      <td>triste cara cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poxa não tem uma negra :( https://t.co/S8U4DGyACx</td>\n",
       "      <td>negativo</td>\n",
       "      <td>poxa negra cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@passarindemarte olha a carinha de assustada d...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>olha carinha assustada ninha cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@ddianatasa Espensif :(</td>\n",
       "      <td>negativo</td>\n",
       "      <td>espensif cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>só falta esse set pra eu ter todos os sets do ...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>falta set pra ter todos sets north cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ischargro De esta forma salimos adelante como...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>forma salimos adelante pais cara triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eu falo que vou dormir cedo e meu projeto vai ...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>falo vou dormir cedo projeto vai agua abaixo c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@gabcrj Nao tem mais e nem vai ter :(. Se eu e...</td>\n",
       "      <td>negativo</td>\n",
       "      <td>nao vai ter cara triste   encontrar alguma ant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    target  \\\n",
       "0  ai quando a gente se conhecer eu vou ficar tão...  negativo   \n",
       "1                    @_iamSHUKRI Esp at 4:30 am :)))  positivo   \n",
       "2              @eujulianaramosv Muito triste cara :(  negativo   \n",
       "3  Poxa não tem uma negra :( https://t.co/S8U4DGyACx  negativo   \n",
       "4  @passarindemarte olha a carinha de assustada d...  negativo   \n",
       "5                            @ddianatasa Espensif :(  negativo   \n",
       "6  só falta esse set pra eu ter todos os sets do ...  negativo   \n",
       "7  @ischargro De esta forma salimos adelante como...  negativo   \n",
       "8  eu falo que vou dormir cedo e meu projeto vai ...  negativo   \n",
       "9  @gabcrj Nao tem mais e nem vai ter :(. Se eu e...  negativo   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ai gente conhecer vou ficar tao feliz aaaaaa v...  \n",
       "1                             esp at   am cara feliz  \n",
       "2                            triste cara cara triste  \n",
       "3                             poxa negra cara triste  \n",
       "4           olha carinha assustada ninha cara triste  \n",
       "5                               espensif cara triste  \n",
       "6     falta set pra ter todos sets north cara triste  \n",
       "7            forma salimos adelante pais cara triste  \n",
       "8  falo vou dormir cedo projeto vai agua abaixo c...  \n",
       "9  nao vai ter cara triste   encontrar alguma ant...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = data.sample(500)\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns=['index'])\n",
    "data['cleaned_text'] = preprocessing(data['text'])\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mechanical-charlotte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adopted-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 76.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['leia'] = data['cleaned_text'].apply(leia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "strong-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#se usar o texto limpo a acurácia fica muito baixa\n",
    "#isso acontece por causa da api não conseguir direito para inglês\n",
    "data['textblob'] = data['text'].apply(textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "transparent-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['oplexion'] = data['cleaned_text'].apply(oplexion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sized-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['sentilex'] = data['cleaned_text'].apply(sentilex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "extensive-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeIA: 0.934\n",
      "TextBlob: 0.924\n",
      "OpLexion: 0.888\n",
      "SentiLex: 0.904\n"
     ]
    }
   ],
   "source": [
    "print(\"LeIA: \" + str(accuracy_score(data['target'], data['leia'])))\n",
    "print(\"TextBlob: \" + str(accuracy_score(data['target'], data['textblob'])))\n",
    "print(\"OpLexion: \" + str(accuracy_score(data['target'], data['oplexion'])))\n",
    "print(\"SentiLex: \" + str(accuracy_score(data['target'], data['sentilex'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127f8c0-00d4-41cd-8b7f-eadfcf79e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['leia'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
